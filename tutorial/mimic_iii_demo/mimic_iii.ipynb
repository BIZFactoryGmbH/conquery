{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90bb6503",
   "metadata": {},
   "source": [
    "# Generate Meta Data and Preprocess Data from the MIMIC-III Demo Dataset for Conquery\n",
    "\n",
    "This tutorial shows how data and meta data tables from the [MIMIC-III Demo Dataset](https://physionet.org/content/mimiciii-demo/1.4/) can be used to prepare data structures\n",
    "needed for conquery.\n",
    "\n",
    "In detail we will generate meta JSONs describing a table schema (Table-JSON), an import operation (Import-JSON, is much like the corresponding Table-JSON used for the preprocessing) and a concept (Concept-JSON, which offers the query functionality) from the table [DIAGNOSES_ICD.csv](https://physionet.org/files/mimiciii-demo/1.4/DIAGNOSES_ICD.csv).\n",
    "\n",
    "Then we will use the Import-JSON to preprocess DIAGNOSES_ICD.csv to a DIAGNOSES_ICD.cqpp (**c**on**q**uery **p**re**p**rocessed).\n",
    "\n",
    "Finally a dataset *MIMIC-III-Demo* will be created in an instance of conquery and the Table-JSON, Concept-JSON and DIAGNOSES_ICD.cqpp will be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8830851",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The imports for this notebook\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests as r\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from enum import Enum, auto\n",
    "from jsonschema import Draft7Validator, RefResolver\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "# Define working directory\n",
    "wd = Path(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4495c0d",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132357b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQTypes(Enum):\n",
    "    STRING = auto()\n",
    "    INTEGER = auto()\n",
    "    BOOLEAN = auto()\n",
    "    REAL= auto()\n",
    "    DECIMAL= auto()\n",
    "    MONEY= auto()\n",
    "    DATE= auto()\n",
    "    DATE_RANGE= auto()\n",
    "\n",
    "def get_csv_name(url):\n",
    "    filename_matcher = re.compile(r\"[\\w\\d_-]+\\.csv\")\n",
    "    match = filename_matcher.search(url)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Unable to extract file name from {url}\")\n",
    "    return match.group(0)\n",
    "\n",
    "\n",
    "def typeConverter(dtype) :\n",
    "    if np.issubdtype(dtype, np.object) :\n",
    "        return CQTypes.STRING.name\n",
    "    if np.issubdtype(dtype, np.integer) :\n",
    "        return CQTypes.INTEGER.name\n",
    "    if np.issubdtype(dtype, np.bool_) :\n",
    "        return CQTypes.BOOLEAN\n",
    "    if np.issubdtype(dtype, np.inexact) :\n",
    "        return CQTypes.REAL\n",
    "    # DECIMAL cannot be derived from the dtype because there is no analogon\n",
    "    # MONEY cannot be derived from the dtype because it is a semantic rather than a logical type\n",
    "    if np.issubdtype(dtype, np.datetime64):\n",
    "        return CQTypes.DATE.name\n",
    "    # DATE_RANGE not supported here yet\n",
    "    raise ValueError(f\"Encountered unhandled dtype: {dtype}\")\n",
    "\n",
    "def generate_table_column(name, dtype) :\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"type\" : typeConverter(dtype)\n",
    "    }\n",
    "\n",
    "def generate_table(name, df, primary_column) :\n",
    "    return {\n",
    "        \"name\" : name,\n",
    "        \"columns\": [ generate_table_column(name, dtype) for name, dtype in zip(df.dtypes.keys().array, df.dtypes.values) if name != primary_column]\n",
    "    }\n",
    "\n",
    "def generate_import_column(name, dtype) :\n",
    "    return {\n",
    "        \"inputColumn\": name,\n",
    "        \"inputType\": typeConverter(dtype),\n",
    "        \"name\": name,\n",
    "        \"operation\": \"COPY\"\n",
    "    }\n",
    "\n",
    "def generate_import(df, primary_column, source_file) :\n",
    "\n",
    "\n",
    "    col_names = list(df.columns.values)\n",
    "    col_names.remove(primary_column)\n",
    "    non_primary_df = data_df[col_names]\n",
    "\n",
    "    # Skip the filename suffix\n",
    "    table_label = source_file.name.split(\".\")[0]\n",
    "\n",
    "    return {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"output\": [ generate_import_column(name, dtype) for name, dtype in zip(non_primary_df.dtypes.keys().array, non_primary_df.dtypes.values)],\n",
    "                \"primary\": {\n",
    "                    **generate_import_column(primary_column, df[[primary_column]].dtypes.values[0]),\n",
    "                    \"required\": True,\n",
    "                },\n",
    "                \"sourceFile\": source_file.as_posix()\n",
    "            }\n",
    "        ],\n",
    "        \"table\": table_label,\n",
    "        \"name\": table_label\n",
    "    }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create a validator from a base schema in the directory \"./json_schema\"\n",
    "\"\"\"\n",
    "def get_validator(base_schema_file):\n",
    "    schema_store = {}\n",
    "\n",
    "    directory = wd / \"json_schema\"\n",
    "        \n",
    "    for file in list(directory.glob(\"*.json\")):\n",
    "        \n",
    "        with open(file, \"r\") as schema_file:\n",
    "            schema = json.load(schema_file)\n",
    "            schema_store[file.name] = schema\n",
    "\n",
    "    resolver = RefResolver.from_schema(schema, store=schema_store)\n",
    "    return Draft7Validator(schema_store[base_schema_file], resolver=resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e3caf1",
   "metadata": {},
   "source": [
    "## Meta Data Creation\n",
    "We will start with the creation of the meta data. For Table-JSON and Import-JSON we need the header of the data table (DIAGNOSES_ICD.csv), we want to use later in conquery.\n",
    "This process is rather generic, as it is usually just an annotation of the columns with type information.\n",
    "\n",
    "For the Concept-JSON we will use the meta data table (D_ICD_DIAGNOSES.csv) to create a tree structured concept from the hierachical *icd9_code*.\n",
    "\n",
    "### Download Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb635a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://physionet.org/files/mimiciii-demo/1.4/DIAGNOSES_ICD.csv?download\"\n",
    "s=r.get(data_url).content\n",
    "data_df = pd.read_csv(io.StringIO(s.decode('utf-8')), index_col=\"row_id\", dtype={\"subject_id\": str, \"hadm_id\": str, \"icd9_code\": str })\n",
    "\n",
    "\n",
    "# Write out the csv because it is needed for the preprocessing\n",
    "data_file = wd / \"data\" / \"csv\" / get_csv_name(data_url)\n",
    "data_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_df.to_csv(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff1e61c",
   "metadata": {},
   "source": [
    "### Generate Table-JSON and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d4243",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = data_file.name.split(\".\")[0]\n",
    "table = generate_table(table_name, data_df, \"subject_id\")\n",
    "\n",
    "get_validator(\"table.json\").validate(table)\n",
    "\n",
    "table_json_file = wd / \"data\" / \"tables\" / f\"{table_name}.table.json\"\n",
    "table_json_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(table_json_file, \"w\") as f:\n",
    "    json.dump(table, f, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc10f7",
   "metadata": {},
   "source": [
    "### Generate Import-JSON and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f12ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = data_file.name.split(\".\")[0]\n",
    "import_ = generate_import(data_df, \"subject_id\", data_file)\n",
    "\n",
    "get_validator(\"import.json\").validate(import_)\n",
    "\n",
    "import_json_file = wd / \"data\" / \"imports\" / f\"{table_name}.import.json\"\n",
    "import_json_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(import_json_file, \"w\") as f:\n",
    "    json.dump(import_, f, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2ed78",
   "metadata": {},
   "source": [
    "### Generate Concept-JSON\n",
    "\n",
    "In this section we generate an ICD concept based in the official ICD-9 catalog (https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Publications/ICD9-CM/2011/Dtab12.zip).\n",
    "We could have used the meta data provided by MIMIC-III (https://physionet.org/files/mimiciii-demo/1.4/D_ICD_DIAGNOSES.csv), but it misses structural and descriptive information (such as chapters, names of higher hierarchies and additional infos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da75f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_url = \"https://physionet.org/files/mimiciii-demo/1.4/D_ICD_DIAGNOSES.csv?download\"\n",
    "s=r.get(meta_url).content\n",
    "meta_df = pd.read_csv(io.StringIO(s.decode('utf-8')), index_col=\"row_id\", dtype={\"subject_id\": str, \"icd9_code\": str })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9fc642",
   "metadata": {},
   "source": [
    "Download and extract the ICD-9 catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e6e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dir = wd / \"data\" / \"meta\"\n",
    "meta_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "meta_url = \"https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Publications/ICD9-CM/2011/Dtab12.zip\"\n",
    "with ZipFile(BytesIO(r.get(meta_url, stream=True).content)) as zip_ref:\n",
    "    zip_ref.extractall(meta_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ff1bd",
   "metadata": {},
   "source": [
    "Read in the rtf format of the catalog. This takes several minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1016e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from striprtf.striprtf import rtf_to_text\n",
    "\n",
    "icd_file = meta_dir / \"dtab12.rtf\"\n",
    "\n",
    "with open(icd_file, \"r\") as f:\n",
    "    icd_catalog = rtf_to_text(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180a862",
   "metadata": {},
   "source": [
    "Parse the catalog into a hierarchical structure that suits conquery as a concept. We define the `concept` foundation and then add the children tree to it.\n",
    "Because the catalog has already the sorted structure we define state variables (`in_chapter`, `in_section`, ...) to keep track of where we have been recently when we insert new nodes to the tree.\n",
    "\n",
    "An ICD-9 code can have up to 5 levels. The first two levels are specified by a range in which a fixed length prefix might fall. All lower sections are determined by a single fixed length prefix. Fortunately, we can distinguish lines in the catalog regarding their level by matching each line to a distinct RegEx schema (`chapter_matcher`, `section_matcher`, ...).\n",
    "The lines that do not match are treated as addition infos and are appended to the last parsed element `in_recent`.\n",
    "If present, we add additional infos to each node, which is displayed in the left column in the frontend. For the ICD-9 codes, this usually includes indication whether the code can be applied using *Exclude* and *Include* sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334849ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_matcher = re.compile(r\"^(?P<chapter>\\d+\\.)\\s*(?P<name>[A-Z, -]+)\\((?P<start>\\d{3})-(?P<end>\\d{3})\\)$\")\n",
    "section_matcher = re.compile(r\"^(?P<name>[A-Z, -]+)\\((?P<start>\\d{3})-(?P<end>\\d{3})\\)$\")\n",
    "subsection_matcher = re.compile(r\"^(?P<prefix>[\\d]{3})\\s+(?P<name>[\\w\\d ()\\-,\\[\\]\\.]+)\")\n",
    "subsubsection_matcher = re.compile(r\"^(?P<prefix>[\\d]{3}\\.\\d)\\s+(?P<name>[\\w\\d ()\\-,\\[\\]\\.]+)\")\n",
    "subsubsubsection_matcher = re.compile(r\"^(?P<prefix>[\\d]{3}\\.\\d{2})\\s+(?P<name>[\\w\\d ()\\-,\\[\\]\\.]+)\")\n",
    "\n",
    "# The concepts builds the root of the hierarchy\n",
    "concept = {\n",
    "    # Placeholder for the rest of the tree\n",
    "    \"children\": [],\n",
    "    # Defines which columns represent codes for this concept\n",
    "    \"connectors\": [{\n",
    "        \"column\": \"DIAGNOSES_ICD.icd9_code\",\n",
    "        \"label\": \"Diagnoses\"\n",
    "    }],\n",
    "    # The display name\n",
    "    \"label\" : \"ICD\",\n",
    "    # The internal name that is used to create an id (must be unique in a dataset among concepts, tables and secondaryIds)\n",
    "    \"name\" : \"icd\",\n",
    "    # Selects define aggregations that create additional columns in the output\n",
    "\t\"selects\": [],\n",
    "    # At the moment there is just this type TREE\n",
    "\t\"type\": \"TREE\"\n",
    "}\n",
    "\n",
    "in_chapter = None\n",
    "in_section = None\n",
    "in_subsection = None\n",
    "in_subsubsection = None\n",
    "in_subsubsubsection = None\n",
    "in_recent = None\n",
    "additional_info_key = \"\"\n",
    "for line in icd_catalog.split(\"\\n\") :\n",
    "\n",
    "    # Chapter\n",
    "    match = chapter_matcher.match(line)\n",
    "    if match:\n",
    "        in_chapter = {\n",
    "            # The label will be displayed in the concept overview, the query editor and the query result\n",
    "            \"label\": f\"{match.group('chapter')}\",\n",
    "            # The description is displayed in the concept overview, the query editor\n",
    "            \"description\": match.group(\"name\").title(),\n",
    "            \"condition\": {\n",
    "                # Defines with codes fall into this chapter/section \n",
    "                \"type\": \"PREFIX_RANGE\",\n",
    "                \"min\": match.group(\"start\"),\n",
    "                \"max\": match.group(\"end\"),\n",
    "            },\n",
    "            # Placeholer for sections, the next level in the tree\n",
    "            \"children\": [],\n",
    "            # Placeholder for \n",
    "            \"additionalInfos\": [],\n",
    "        }\n",
    "        in_recent = in_chapter\n",
    "        # Reset sub-levels\n",
    "        in_section = None\n",
    "        in_subsection = None\n",
    "        in_subsubsection = None\n",
    "        in_subsubsubsection = None\n",
    "        additional_info_key = \"\"\n",
    "        concept[\"children\"].append(in_chapter)\n",
    "        continue\n",
    "\n",
    "    # Section\n",
    "    match = section_matcher.match(line)\n",
    "    if match:\n",
    "        in_section = {\n",
    "            \"label\": f\"{match.group('start')}-{match.group('end')}\",\n",
    "            \"description\": match.group(\"name\").title(),\n",
    "            \"condition\": {\n",
    "                \"type\": \"PREFIX_RANGE\",\n",
    "                \"min\": match.group(\"start\"),\n",
    "                \"max\": match.group(\"end\"),\n",
    "            },\n",
    "            \"children\": [],\n",
    "            \"additionalInfos\": [],\n",
    "        }\n",
    "        in_recent = in_section\n",
    "        in_subsection = None\n",
    "        in_subsubsection = None\n",
    "        in_subsubsubsection = None\n",
    "        additional_info_key = \"\"\n",
    "        in_chapter[\"children\"].append(in_section)\n",
    "        continue\n",
    "\n",
    "    # Subsection\n",
    "    match = subsection_matcher.match(line)\n",
    "    if match:\n",
    "        in_subsection = {\n",
    "            \"label\": match.group(\"prefix\"),\n",
    "            \"description\": match.group(\"name\"),\n",
    "            \"condition\": {\n",
    "                \"type\": \"PREFIX_LIST\",\n",
    "                \"prefixes\": [match.group(\"prefix\")],\n",
    "            },\n",
    "            \"children\": [],\n",
    "            \"additionalInfos\": [],\n",
    "        }\n",
    "        in_recent = in_subsection\n",
    "        in_subsubsection = None\n",
    "        in_subsubsubsection = None\n",
    "        additional_info_key = \"\"\n",
    "\n",
    "        upper_level = in_section or in_chapter\n",
    "        upper_level[\"children\"].append(in_subsection)\n",
    "        continue\n",
    "\n",
    "    # Subsubsection\n",
    "    match = subsubsection_matcher.match(line)\n",
    "    if match:\n",
    "        in_subsubsection = {\n",
    "            \"label\": match.group(\"prefix\"),\n",
    "            \"description\": match.group(\"name\"),\n",
    "            \"condition\": {\n",
    "                \"type\": \"PREFIX_LIST\",\n",
    "                # the descriptive codes differ from the codes in the data in that they contain a dot that we strip\n",
    "                \"prefixes\": [match.group(\"prefix\").replace(\".\",\"\")],\n",
    "            },\n",
    "            \"children\": [],\n",
    "            \"additionalInfos\": [],\n",
    "        }\n",
    "        in_recent = in_subsubsection\n",
    "        in_subsubsubsection = None\n",
    "        additional_info_key = \"\"\n",
    "\n",
    "        upper_level = in_subsection or in_section or in_chapter\n",
    "        upper_level[\"children\"].append(in_subsubsection)\n",
    "        continue\n",
    "\n",
    "    # Subsubsubsection\n",
    "    match = subsubsubsection_matcher.match(line)\n",
    "    if match:\n",
    "        in_subsubsubsection = {\n",
    "            \"label\": match.group(\"prefix\"),\n",
    "            \"description\": match.group(\"name\"),\n",
    "            \"condition\": {\n",
    "                \"type\": \"PREFIX_LIST\",\n",
    "                \"prefixes\": [match.group(\"prefix\").replace(\".\",\"\")],\n",
    "            },\n",
    "            \"children\": [],\n",
    "            \"additionalInfos\": [],\n",
    "        }\n",
    "        in_recent = in_subsubsubsection\n",
    "        additional_info_key = \"\"\n",
    "\n",
    "        \n",
    "        upper_level = in_subsubsection or in_subsection or in_section or in_chapter\n",
    "        upper_level[\"children\"].append(in_subsubsubsection)\n",
    "        continue\n",
    "\n",
    "    # Additional Infos\n",
    "    if not in_recent:\n",
    "        continue\n",
    "    value = line\n",
    "    if line.startswith('Excludes:'):\n",
    "        additional_info_key = \"Excludes:\"\n",
    "        # +1 for the \\t \n",
    "        value = value[len(additional_info_key)+1:]\n",
    "    elif line.startswith('Includes:'):\n",
    "        additional_info_key = \"Includes:\"\n",
    "        value = value[len(additional_info_key)+1:]\n",
    "\n",
    "    additional_info = in_recent[\"additionalInfos\"]\n",
    "\n",
    "    # Should be one item at max\n",
    "    items = list(filter(lambda i: i[\"key\"] == additional_info_key, additional_info))\n",
    "\n",
    "    if len(items) > 1:\n",
    "        raise RuntimeError(f\"Expected key {additional_info_key} to appear at most once\")\n",
    "    if len(items) == 0:\n",
    "        # First time this key appeared\n",
    "        additional_info.append(\n",
    "            {\n",
    "                \"key\": additional_info_key,\n",
    "                \"value\": value\n",
    "            }\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    items[0][\"value\"] += f\"\\n{value}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca3fb2",
   "metadata": {},
   "source": [
    "Validate and write the concept. //TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e498893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concept_json_file = wd / \"data\" / \"concepts\" / \"icd.concept.json\"\n",
    "concept_json_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(concept_json_file, \"w\") as f:\n",
    "    json.dump(concept, f, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf4b40",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "This step assumes that the backend has already been build/compiled (using the script `conquery/scripts/build_no_version.sh`).\n",
    "\n",
    "The command that is executed converts the CSV we downloaded initially to an CQPP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c16b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wd / \"data\" / \"cqpp\").mkdir(exist_ok=True, parents=True)\n",
    "!java -jar ../../executable/target/executable-0.0.0-SNAPSHOT.jar preprocess --desc ./data/import* --in . --out ./data/cqpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b6ec9",
   "metadata": {},
   "source": [
    "## Import/Upload\n",
    "\n",
    "This step assumes, that the backend is running and the admin endpoint is reachable under the specified url (e.g. using the script `conquery/scripts/run_conquery_cypress.sh`).\n",
    "\n",
    "### Meta Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f713c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create Dataset\n",
    "datasetId = \"mimic-iii-demo\"\n",
    "r.post(\"http://localhost:8081/admin/datasets\", json={\"name\":\"mimic-iii-demo\", \"label\": \"MIMIC-III Demo\"}, headers={\"content-type\":\"application/json\"})\n",
    "time.sleep(2)\n",
    "r.post(f\"http://localhost:8081/admin/datasets/{datasetId}/tables\", json=table, headers={\"content-type\":\"application/json\"})\n",
    "time.sleep(2)\n",
    "r.post(f\"http://localhost:8081/admin/datasets/{datasetId}/concepts\", json=concept, headers={\"content-type\":\"application/json\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f8121",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee580eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wd / \"data\" / \"cqpp\" / \"DIAGNOSES_ICD.cqpp\", \"rb\") as f :\n",
    "    r.post(f\"http://localhost:8081/admin/datasets/{datasetId}/cqpp\", data= f, headers={\"content-type\":\"application/octet-stream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444adaa",
   "metadata": {},
   "source": [
    "### Update Matching Stats\n",
    "\n",
    "This action collects statistics that are displayed in the frontend when hovering over concepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r.post(f\"http://localhost:8081/admin/datasets/{datasetId}/update-matching-stats\", headers={\"content-type\":\"application/json\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b609d",
   "metadata": {},
   "source": [
    "## Visit the Frontend\n",
    "\n",
    "Finally you can access the frontend under the url http://localhost:8000/?access_token=user.SUPERUSER@SUPERUSER as the super user. In the top right corner choose the *MIMIC-III Demo* dataset. You can then start combining nodes of the *ICD* concept in the query editor and submit your query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
