{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90bb6503",
   "metadata": {},
   "source": [
    "# Generate Meta Data and Preprocess Data from the MIMIC-III Demo Dataset for Conquery\n",
    "\n",
    "This tutorial shows how data and meta data tables from the [MIMIC-III Demo Dataset](https://physionet.org/content/mimiciii-demo/1.4/) can be used to prepare data structures\n",
    "needed for conquery.\n",
    "\n",
    "In detail we will generate meta JSONs describing a table schema (Table-JSON), an import operation (Import-JSON, is much like the corresponding Table-JSON used for the preprocessing) and a concept (Concept-JSON, which offers the query functionality) from the tables [DIAGNOSES_ICD.csv](https://physionet.org/files/mimiciii-demo/1.4/DIAGNOSES_ICD.csv) and [D_ICD_DIAGNOSES.csv](https://physionet.org/content/mimiciii-demo/1.4/D_ICD_DIAGNOSES.csv).\n",
    "\n",
    "Then we will use the Import-JSON to preprocess DIAGNOSES_ICD.csv to a DIAGNOSES_ICD.cqpp (**c**on**q**uery **p**re**p**rocessed).\n",
    "\n",
    "Finally a dataset *MIMIC-III-Demo* will be created in an instance of conquery and the Table-JSON, Concept-JSON and DIAGNOSES_ICD.cqpp will be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8830851",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The imports for this notebook\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests as r\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from enum import Enum, auto\n",
    "from jsonschema import Draft7Validator, RefResolver\n",
    "from pathlib import Path\n",
    "\n",
    "# Define working directory\n",
    "wd = Path(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4495c0d",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "132357b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQTypes(Enum):\n",
    "    STRING = auto()\n",
    "    INTEGER = auto()\n",
    "    BOOLEAN = auto()\n",
    "    REAL= auto()\n",
    "    DECIMAL= auto()\n",
    "    MONEY= auto()\n",
    "    DATE= auto()\n",
    "    DATE_RANGE= auto()\n",
    "\n",
    "def get_csv_name(url):\n",
    "    filename_matcher = re.compile(r\"[\\w\\d_-]+\\.csv\")\n",
    "    match = filename_matcher.search(url)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Unable to extract file name from {url}\")\n",
    "    return match.group(0)\n",
    "\n",
    "\n",
    "def typeConverter(dtype) :\n",
    "    if np.issubdtype(dtype, np.object) :\n",
    "        return CQTypes.STRING.name\n",
    "    if np.issubdtype(dtype, np.integer) :\n",
    "        return CQTypes.INTEGER.name\n",
    "    if np.issubdtype(dtype, np.bool_) :\n",
    "        return CQTypes.BOOLEAN\n",
    "    if np.issubdtype(dtype, np.inexact) :\n",
    "        return CQTypes.REAL\n",
    "    # DECIMAL cannot be derived from the dtype because there is no analogon\n",
    "    # MONEY cannot be derived from the dtype because it is a semantic rather than a logical type\n",
    "    if np.issubdtype(dtype, np.datetime64):\n",
    "        return CQTypes.DATE.name\n",
    "    # DATE_RANGE not supported here yet\n",
    "    raise ValueError(f\"Encountered unhandled dtype: {dtype}\")\n",
    "\n",
    "def generate_table_column(name, dtype) :\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"type\" : typeConverter(dtype)\n",
    "    }\n",
    "\n",
    "def generate_table(name, df) :\n",
    "    return {\n",
    "        \"name\" : name,\n",
    "        \"columns\": [ generate_table_column(name, dtype) for name, dtype in zip(df.dtypes.keys().array, df.dtypes.values)]\n",
    "    }\n",
    "\n",
    "def generate_import_column(name, dtype) :\n",
    "    return {\n",
    "        \"inputColumn\": name,\n",
    "        \"inputType\": typeConverter(dtype),\n",
    "        \"name\": name,\n",
    "        \"operation\": \"COPY\"\n",
    "    }\n",
    "\n",
    "def generate_import(df, primary_column, source_file) :\n",
    "\n",
    "\n",
    "    col_names = list(df.columns.values)\n",
    "    col_names.remove(primary_column)\n",
    "    non_primary_df = data_df[col_names]\n",
    "\n",
    "    # Skip the filename suffix\n",
    "    table_label = source_file.name.split(\".\")[0]\n",
    "\n",
    "    return {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"output\": [ generate_import_column(name, dtype) for name, dtype in zip(non_primary_df.dtypes.keys().array, non_primary_df.dtypes.values)],\n",
    "                \"primary\": {\n",
    "                    **generate_import_column(primary_column, df[[primary_column]].dtypes.values[0]),\n",
    "                    \"required\": True,\n",
    "                },\n",
    "                \"sourceFile\": source_file.as_posix()\n",
    "            }\n",
    "        ],\n",
    "        \"table\": table_label,\n",
    "        \"name\": table_label\n",
    "    }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create a validator from a base schema in the directory \"./json_schema\"\n",
    "\"\"\"\n",
    "def get_validator(base_schema_file):\n",
    "    schema_store = {}\n",
    "\n",
    "    directory = wd / \"json_schema\"\n",
    "        \n",
    "    for file in list(directory.glob(\"*.json\")):\n",
    "        \n",
    "        with open(file, \"r\") as schema_file:\n",
    "            schema = json.load(schema_file)\n",
    "            schema_store[file.name] = schema\n",
    "\n",
    "    resolver = RefResolver.from_schema(schema, store=schema_store)\n",
    "    return Draft7Validator(schema_store[base_schema_file], resolver=resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e3caf1",
   "metadata": {},
   "source": [
    "## Meta Data Creation\n",
    "We will start with the creation of the meta data. For Table-JSON and Import-JSON we need the header of the data table (DIAGNOSES_ICD.csv), we want to use later in conquery.\n",
    "This process is rather generic, as it is usually just an annotation of the columns with type information.\n",
    "\n",
    "For the Concept-JSON we will use the meta data table (D_ICD_DIAGNOSES.csv) to create a tree structured concept from the hierachical *icd9_code*.\n",
    "\n",
    "### Download Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0eb635a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://physionet.org/files/mimiciii-demo/1.4/DIAGNOSES_ICD.csv?download\"\n",
    "s=r.get(data_url).content\n",
    "data_df = pd.read_csv(io.StringIO(s.decode('utf-8')), index_col=\"row_id\", dtype={\"subject_id\": str, \"hadm_id\": str, \"icd9_code\": str })\n",
    "\n",
    "\n",
    "# Write out the csv because it is needed for the preprocessing\n",
    "data_file = wd / \"data\" / \"csv\" / get_csv_name(data_url)\n",
    "data_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_df.to_csv(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff1e61c",
   "metadata": {},
   "source": [
    "### Generate Table-JSON and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "071d4243",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = data_file.name.split(\".\")[0]\n",
    "table = generate_table(table_name, data_df)\n",
    "\n",
    "get_validator(\"table.json\").validate(table)\n",
    "\n",
    "table_json_file = wd / \"data\" / \"tables\" / f\"{table_name}.table.json\"\n",
    "table_json_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(table_json_file, \"w\") as f:\n",
    "    json.dump(table, f, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc10f7",
   "metadata": {},
   "source": [
    "### Generate Import-JSON and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "67f12ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = data_file.name.split(\".\")[0]\n",
    "import_ = generate_import(data_df, \"subject_id\", data_file)\n",
    "\n",
    "get_validator(\"import.json\").validate(import_)\n",
    "\n",
    "import_json_file = wd / \"data\" / \"imports\" / f\"{table_name}.import.json\"\n",
    "import_json_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(import_json_file, \"w\") as f:\n",
    "    json.dump(import_, f, indent=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
